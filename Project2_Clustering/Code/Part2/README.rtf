{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fmodern\fcharset0 CourierNewPSMT;\f1\froman\fcharset0 TimesNewRomanPSMT;}
{\colortbl;\red255\green255\blue255;\red51\green51\blue51;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid101\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid201\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid301\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid4}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}}
\margl1440\margr1440\vieww15800\viewh13060\viewkind0
\deftab720
\pard\pardeftab720\ri0

\f0\fs20 \cf0 Files:\
c_cho.txt		   -the initial centers of cho.txt from kmeans++\
c_iyer.txt  	   -the initial centers of iyer.txt from kmeans++\
c_new_dataset.txt	   -the initial centers of new_dataset_1.txt from kmeans++\
\pard\pardeftab720\li2160\fi-2160\ri0
\cf0 cho_data.txt	   -attributes of all points in cho.txt\
iyer_data.txt	   -attributes of all points in iyer.txt\
new_dataset_1_data.txt  -attributes of all points in new_dataset_1.txt\
kmeans_mapper.py	   -the mapper for Mapreduce Hadoop streaming\
kmeans_reducer.py    -the reducer for Mapreduce Hadoop streaming\
\pard\pardeftab720\ri0
\cf0 \
\pard\pardeftab720\ri0\sl276\slmult1
\cf0 How to run k-means MapReduce in Hadoop streaming\
\pard\pardeftab720\li720\fi-360\ri0\sl276\slmult1
\ls1\ilvl0\cf0 1.	Using k-means++ to generate initial centers init_centroid.txt as the initial centers for the k-means MapReduce. The init_centroid.txt already generated, which are c_cho.txt, c_iyer.txt, and new_dataset_1_data.txt\
2.	Configuration of single-node cluster\
3.	Format filesystem and start NameNode daemon and DataNode daemon\
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\li720\ri0\sl276\slmult1
\cf2 $ bin/hdfs namenode \'96format\
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\ri0\sl276\slmult1
\cf2       $ sbin/start-dfs.sh\
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\li720\fi-360\ri0\sl276\slmult1
\ls2\ilvl0\cf2 4.	Make the HDFS directories required to execute Mapreduce jobs:
\f1 \
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\li720\ri0\sl276\slmult1

\f0 \cf2 $ bin/hdfs dfs -mkdir /user\
$ bin/hdfs dfs -mkdir /user/<username>\
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\li720\fi-360\ri0\sl276\slmult1
\ls3\ilvl0\cf2 5.	Copy the input files into the distributed filesystem:
\f1 \
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\li720\ri0\sl276\slmult1

\f0 \cf2 $ cp ~/cse601/mapper.py /usr/local/hadoop
\f1 \

\f0 $ cp ~/cse601/reducer.py /usr/local/hadoop
\f1 \

\f0 $ bin/hdfs dfs \'96put init_centroid.txt
\f1 \

\f0 $ bin/hdfs dfs \'96put data.txt\
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\li720\fi-360\ri0\sl276\slmult1
\ls4\ilvl0\cf2 6.	Writing a shell script to do the MapReduce job-chain:\
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\li720\ri0\sl276\slmult1
\cf2 $ CACHE=init_centroid.txt#centers\
$ for i in \{1..20\}\
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\fi760\ri0\sl276\slmult1
\cf2 >do\
>echo $i\
>OUTPUT=centroid_iter_$i\
>bin/hadoop jar share/hadoop/tools/lib/hadoop-streaming-2.8.1.jar\\\
>-input data.txt \'96output $OUTPUT\\\
>-mapper mapper.py \'96reducer reducer.py\\\
>-file mapper.py \'96file reducer.py\\\
>-cacheFile $CACHE \'96numReduceTasks 1;\
>CACHE=$OUTPUT/part-00000#centers\
>echo $CACHE\
>bin/hdfs dfs \'96cat $OUTPUT/*\
>done
\f1 \
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\ri0\sl276\slmult1

\f0 \cf2    7. Copy the outputs from HDFS to local
\f1 \
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\fi720\ri0\sl276\slmult1

\f0 \cf2 $ for i in \{1..20\}\
>do
\f1 \

\f0 >bin/hdfs dfs \'96get centroid_iter_$i ~/cse601/centroid_iter_$i
\f1 \

\f0 >done
\f1 \
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\ri0\sl276\slmult1

\f0 \cf2    8. When you\'92re done, stop the daemons with:
\f1 \

\f0       $ sbin/stop-dfs.sh  
\f1 \
}